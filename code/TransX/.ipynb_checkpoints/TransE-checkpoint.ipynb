{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Generally, there are several steps as following:\n",
    "\n",
    "1. Transform raw data to standard format(CSV), some supported format lists as following(When dealing with the raw data, several exceptions should be considered):\n",
    "    1. There exists blank lines, inconsistent columns(!=3)\n",
    "    2. Head, relation, tail is nan or string that only contains space\n",
    "    3. Some string will be parsed as nan, exp.NAN, N/A, NA and so on\n",
    "2. Use data of standard format to generate entity and relation dict\n",
    "3. Split train, evaluation and test data if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "from config import Config\n",
    "\n",
    "from utils import utils\n",
    "from models import TransE, TransH, TransA, TransD, KG2E\n",
    "from utils import evaluation\n",
    "from dataloader.dataloader import tripleDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING : Path ../data/TransX/model/ does not exist!\n",
      "INFO : Creating path ../data/TransX/model/.\n",
      "INFO : Successfully making dir!\n",
      "WARNING : Path ../data/TransX/log/TransE/ does not exist!\n",
      "INFO : Creating path ../data/TransX/log/TransE/.\n",
      "INFO : Successfully making dir!\n",
      "WARNING : Path ../data/TransX/embed/ does not exist!\n",
      "INFO : Creating path ../data/TransX/embed/.\n",
      "INFO : Successfully making dir!\n"
     ]
    }
   ],
   "source": [
    "from config import Config\n",
    "default_conf = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_process(raw_path, save_path, names=None, header=None, sep=\"\\t\", encoding=\"utf-8\", compression=\"infer\"):\n",
    "    print(\"INFO : Loading data of type %s\" % os.path.splitext(raw_path)[-1])\n",
    "    raw_df = pd.read_csv(raw_path,\n",
    "                        sep=sep,\n",
    "                        encoding=encoding,\n",
    "                        names=names,\n",
    "                        header=header,\n",
    "                        keep_default_na=False,  # ==> Solve default nan\n",
    "                        compression=compression,# ==> Solve ZIP and TAR\n",
    "                        error_bad_lines=False,  # ==> Solve inconsistent lines\n",
    "                        warn_bad_lines=False,   # ==> Solve inconsistent lines\n",
    "                        skip_blank_lines=True)  # ==> Solve blank lines\n",
    "    print(\"INFO : Remove the space from the head and tail of entity.\")\n",
    "    raw_df = raw_df.applymap(lambda x: x.strip())  # Rid of the space in head and tail of entity\n",
    "    print(\"INFO : Drop line with nan value.\")    # Attention: \" \" should be removed.\n",
    "    raw_df.replace({'': np.nan}, inplace=True)\n",
    "    raw_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    print(\"INFO : Save standard data to file path : %s\" % save_path)\n",
    "    raw_df.to_csv(save_path, sep=\"\\t\", header=None, index=None, encoding=\"utf-8\")\n",
    "    print(\"INFO : Successfully saving!\")\n",
    "    return raw_df\n",
    "\n",
    "\n",
    "def generate_dict(data_dfs, dict_save_dir):\n",
    "\n",
    "    raw_df = pd.concat(data_dfs, axis=0)\n",
    "    raw_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    head_counter = Counter(raw_df[\"head\"])\n",
    "    tail_counter = Counter(raw_df[\"tail\"])\n",
    "    rela_counter = Counter(raw_df[\"relation\"])\n",
    "\n",
    "    # Generate entity and relation list\n",
    "    entity_list = list((head_counter + tail_counter).keys())\n",
    "    rela_list = list(rela_counter.keys())\n",
    "\n",
    "    # Transform to index dict\n",
    "    print(\"INFO : Transform to index dict\")\n",
    "    entity_dict = dict([(word, ind) for ind, word in enumerate(entity_list)])\n",
    "    rela_dict = dict([(word, ind) for ind, word in enumerate(rela_list)])\n",
    "\n",
    "    # Save path\n",
    "    entity_dict_path = os.path.join(dict_save_dir, \"entity_dict.json\")\n",
    "    rela_dict_path = os.path.join(dict_save_dir, \"relation_dict.json\")\n",
    "\n",
    "    # Saving dicts\n",
    "    json.dump({\"stoi\": entity_dict, \"itos\": entity_list}, open(entity_dict_path, \"w\"))\n",
    "    json.dump({\"stoi\": rela_dict, 'itos': rela_list}, open(rela_dict_path, \"w\"))\n",
    "\n",
    "    return {\"Entity\": {\"stoi\": entity_dict, \"itos\": entity_list}, \n",
    "            \"Rela\": {\"stoi\": rela_dict, 'itos': rela_list} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Loading data of type .txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_2793/753348521.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  raw_df = pd.read_csv(raw_path,\n",
      "/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_2793/753348521.py:3: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  raw_df = pd.read_csv(raw_path,\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m test_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/test.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step1: Transform raw data to standard format\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mcsv_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtail\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m valid_df \u001b[38;5;241m=\u001b[39m csv_process(raw_path\u001b[38;5;241m=\u001b[39mvalid_file,\n\u001b[1;32m     13\u001b[0m                 save_path\u001b[38;5;241m=\u001b[39mdefault_conf\u001b[38;5;241m.\u001b[39mvalid_path,\n\u001b[1;32m     14\u001b[0m                 names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m                 header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m                 sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m                 encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m test_df \u001b[38;5;241m=\u001b[39m csv_process(raw_path\u001b[38;5;241m=\u001b[39mtest_file,\n\u001b[1;32m     19\u001b[0m                 save_path\u001b[38;5;241m=\u001b[39mdefault_conf\u001b[38;5;241m.\u001b[39mtest_path,\n\u001b[1;32m     20\u001b[0m                 names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     21\u001b[0m                 header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m                 sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m                 encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mcsv_process\u001b[0;34m(raw_path, save_path, names, header, sep, encoding, compression)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcsv_process\u001b[39m(raw_path, save_path, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO : Loading data of type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(raw_path)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m     raw_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ==> Solve default nan\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# ==> Solve ZIP and TAR\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merror_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ==> Solve inconsistent lines\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mwarn_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# ==> Solve inconsistent lines\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mskip_blank_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ==> Solve blank lines\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO : Remove the space from the head and tail of entity.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     raw_df \u001b[38;5;241m=\u001b[39m raw_df\u001b[38;5;241m.\u001b[39mapplymap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mstrip())  \u001b[38;5;66;03m# Rid of the space in head and tail of entity\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/train.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "dict_dir = \"../../data/TransX\"\n",
    "train_file = \"../../data/TransX/train.txt\"\n",
    "valid_file = \"../../data/TransX/valid.txt\"\n",
    "test_file = \"../../data/TransX/test.txt\"\n",
    "# Step1: Transform raw data to standard format\n",
    "train_df = csv_process(raw_path=train_file,\n",
    "                save_path=default_conf.pos_path,\n",
    "                names=[\"head\", \"relation\", \"tail\"],\n",
    "                header=None,\n",
    "                sep=\"\\t\",\n",
    "                encoding=\"utf-8\")\n",
    "valid_df = csv_process(raw_path=valid_file,\n",
    "                save_path=default_conf.valid_path,\n",
    "                names=[\"head\", \"relation\", \"tail\"],\n",
    "                header=None,\n",
    "                sep=\"\\t\",\n",
    "                encoding=\"utf-8\")\n",
    "test_df = csv_process(raw_path=test_file,\n",
    "                save_path=default_conf.test_path,\n",
    "                names=[\"head\", \"relation\", \"tail\"],\n",
    "                header=None,\n",
    "                sep=\"\\t\",\n",
    "                encoding=\"utf-8\")\n",
    "\n",
    "entity_rela_dict = generate_dict(data_dfs=[train_df, valid_df, test_df],\n",
    "                 dict_save_dir=dict_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Entity', 'Rela'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_rela_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1345, 14951)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_rela_dict[\"Rela\"][\"stoi\"]), len(entity_rela_dict[\"Entity\"][\"stoi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Load entity and relation dict.\n",
      "INFO : Loading positive triples and transform to index.\n",
      "INFO : Generate negtive samples from positive samples.\n",
      "INFO : Load entity and relation dict.\n",
      "INFO : Loading positive triples and transform to index.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize dataset and dataloader\n",
    "# If print(dataset[:]), you can get the result like:\n",
    "#   (np.array(N, 3, dtype=int64), np.array(N, 3, dtype=int64))\n",
    "# The first array represents the positive triples, while\n",
    "#   the second array represents the negtive ones.\n",
    "#   N is the size of all data.\n",
    "repSeed = 0\n",
    "exSeed = 0\n",
    "headSeed = 0\n",
    "tailSeed = 0\n",
    "\n",
    "train_dataset = tripleDataset(posDataPath=default_conf.pos_path,\n",
    "                    entityDictPath=default_conf.ent_path,\n",
    "                    relationDictPath=default_conf.rel_path)\n",
    "\n",
    "train_dataset.generateNegSamples(repProba=default_conf.rep_proba,\n",
    "                        exProba=default_conf.ex_proba,\n",
    "                        repSeed=repSeed,\n",
    "                        exSeed=exSeed,\n",
    "                        headSeed=headSeed,\n",
    "                        tailSeed=tailSeed)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                    batch_size=default_conf.batch_size,\n",
    "                    shuffle=default_conf.shuffle,\n",
    "                    num_workers=default_conf.num_workers,\n",
    "                    drop_last=default_conf.drop_last)\n",
    "\n",
    "\n",
    "valid_dataset = tripleDataset(posDataPath=default_conf.valid_path,\n",
    "                        entityDictPath=default_conf.ent_path,\n",
    "                        relationDictPath=default_conf.rel_path)\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                        batch_size=len(valid_dataset),\n",
    "                        shuffle=False,\n",
    "                        drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6476,    53,   121],\n",
       "        [ 9899,    37,   840],\n",
       "        [10779,    22,  1313],\n",
       "        ...,\n",
       "        [ 2557,    11,  1079],\n",
       "        [ 7059,   290,  4339],\n",
       "        [ 5528,   119,   589]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9046,   53,  121],\n",
       "        [9899,   37, 4969],\n",
       "        [4300,   22, 1313],\n",
       "        ...,\n",
       "        [2557,   11, 4211],\n",
       "        [7059,  290,   29],\n",
       "        [2764,  119,  589]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_dataloader(seed):\n",
    "    epSeed = seed\n",
    "    exSeed = seed\n",
    "    headSeed = seed\n",
    "    tailSeed = seed\n",
    "\n",
    "    train_dataset = tripleDataset(posDataPath=default_conf.pos_path,\n",
    "                        entityDictPath=default_conf.ent_path,\n",
    "                        relationDictPath=default_conf.rel_path)\n",
    "\n",
    "    train_dataset.generateNegSamples(repProba=default_conf.rep_proba,\n",
    "                            exProba=default_conf.ex_proba,\n",
    "                            repSeed=repSeed,\n",
    "                            exSeed=exSeed,\n",
    "                            headSeed=headSeed,\n",
    "                            tailSeed=tailSeed)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                        batch_size=default_conf.batch_size,\n",
    "                        shuffle=default_conf.shuffle,\n",
    "                        num_workers=default_conf.num_workers,\n",
    "                        drop_last=default_conf.drop_last)\n",
    "    \n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def prepare_val_datset():\n",
    "    valid_dataset = tripleDataset(posDataPath=default_conf.valid_path,\n",
    "                        entityDictPath=default_conf.ent_path,\n",
    "                        relationDictPath=default_conf.rel_path)\n",
    "    valid_dataloader = DataLoader(valid_dataset,\n",
    "                            batch_size=len(valid_dataset),\n",
    "                            shuffle=False,\n",
    "                            drop_last=False)\n",
    "\n",
    "    return valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransE.TransE(entityNum=len(entity_rela_dict[\"Entity\"][\"stoi\"]),\n",
    "                    relationNum=len(entity_rela_dict[\"Rela\"][\"stoi\"]),\n",
    "                    embeddingDim=default_conf.TransE[\"EmbeddingDim\"],\n",
    "                    margin=default_conf.TransE[\"Margin\"],\n",
    "                    L=default_conf.TransE[\"L\"])\n",
    "model.to(default_conf.device)\n",
    "                    \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                weight_decay= default_conf.weight_decay,\n",
    "                                lr=default_conf.learning_rate)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=default_conf.lr_decay, patience=default_conf.lr_decay_epoch, threshold=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Using seed 0\n",
      "INFO : Load entity and relation dict.\n",
      "INFO : Loading positive triples and transform to index.\n",
      "INFO : Generate negtive samples from positive samples.\n",
      "====================EPOCHS(1/5)====================\n",
      "[TRAIN-EPOCH(1/5)-STEP(0/483142)]Loss:0.9982, minLoss:0.9982\n",
      "[TRAIN-EPOCH(1/5)-STEP(1/483142)]Loss:0.9924, minLoss:0.9924\n",
      "[TRAIN-EPOCH(1/5)-STEP(2/483142)]Loss:0.9967, minLoss:0.9924\n",
      "[TRAIN-EPOCH(1/5)-STEP(3/483142)]Loss:0.9817, minLoss:0.9817\n",
      "[TRAIN-EPOCH(1/5)-STEP(4/483142)]Loss:0.9732, minLoss:0.9732\n",
      "[TRAIN-EPOCH(1/5)-STEP(5/483142)]Loss:0.9750, minLoss:0.9732\n",
      "====================EPOCHS(2/5)====================\n",
      "[TRAIN-EPOCH(2/5)-STEP(0/483142)]Loss:0.9684, minLoss:0.9684\n",
      "[TRAIN-EPOCH(2/5)-STEP(1/483142)]Loss:0.9678, minLoss:0.9678\n",
      "[TRAIN-EPOCH(2/5)-STEP(2/483142)]Loss:0.9613, minLoss:0.9613\n",
      "[TRAIN-EPOCH(2/5)-STEP(3/483142)]Loss:0.9583, minLoss:0.9583\n",
      "[TRAIN-EPOCH(2/5)-STEP(4/483142)]Loss:0.9478, minLoss:0.9478\n",
      "[TRAIN-EPOCH(2/5)-STEP(5/483142)]Loss:0.9364, minLoss:0.9364\n",
      "====================EPOCHS(3/5)====================\n",
      "[TRAIN-EPOCH(3/5)-STEP(0/483142)]Loss:0.9372, minLoss:0.9364\n",
      "[TRAIN-EPOCH(3/5)-STEP(1/483142)]Loss:0.9346, minLoss:0.9346\n",
      "[TRAIN-EPOCH(3/5)-STEP(2/483142)]Loss:0.9369, minLoss:0.9346\n",
      "[TRAIN-EPOCH(3/5)-STEP(3/483142)]Loss:0.9327, minLoss:0.9327\n",
      "[TRAIN-EPOCH(3/5)-STEP(4/483142)]Loss:0.9305, minLoss:0.9305\n",
      "[TRAIN-EPOCH(3/5)-STEP(5/483142)]Loss:0.9033, minLoss:0.9033\n",
      "====================EPOCHS(4/5)====================\n",
      "[TRAIN-EPOCH(4/5)-STEP(0/483142)]Loss:0.9124, minLoss:0.9033\n",
      "[TRAIN-EPOCH(4/5)-STEP(1/483142)]Loss:0.9025, minLoss:0.9025\n",
      "[TRAIN-EPOCH(4/5)-STEP(2/483142)]Loss:0.9039, minLoss:0.9025\n",
      "[TRAIN-EPOCH(4/5)-STEP(3/483142)]Loss:0.8968, minLoss:0.8968\n",
      "[TRAIN-EPOCH(4/5)-STEP(4/483142)]Loss:0.8878, minLoss:0.8878\n",
      "[TRAIN-EPOCH(4/5)-STEP(5/483142)]Loss:0.8836, minLoss:0.8836\n",
      "====================EPOCHS(5/5)====================\n",
      "[TRAIN-EPOCH(5/5)-STEP(0/483142)]Loss:0.8968, minLoss:0.8836\n",
      "[TRAIN-EPOCH(5/5)-STEP(1/483142)]Loss:0.8852, minLoss:0.8836\n",
      "[TRAIN-EPOCH(5/5)-STEP(2/483142)]Loss:0.8610, minLoss:0.8610\n",
      "[TRAIN-EPOCH(5/5)-STEP(3/483142)]Loss:0.8744, minLoss:0.8610\n",
      "[TRAIN-EPOCH(5/5)-STEP(4/483142)]Loss:0.8673, minLoss:0.8610\n",
      "[TRAIN-EPOCH(5/5)-STEP(5/483142)]Loss:0.8560, minLoss:0.8560\n"
     ]
    }
   ],
   "source": [
    "sumWriter = SummaryWriter(log_dir=default_conf.summary_dir)\n",
    "\n",
    "# Training, GLOBALSTEP and GLOBALEPOCH are used for summary\n",
    "minLoss = float(\"inf\")\n",
    "bestMR = float(\"inf\")\n",
    "\n",
    "GLOBALSTEP = 0\n",
    "GLOBALEPOCH = 0\n",
    "\n",
    "for seed in range(1):\n",
    "    print(\"INFO : Using seed %d\" % seed)\n",
    "\n",
    "    train_dataloader = prepare_train_dataloader(seed)\n",
    "\n",
    "    for epoch in range(default_conf.epochs):\n",
    "        GLOBALEPOCH += 1\n",
    "        STEP = 0\n",
    "        print(\"=\"*20+\"EPOCHS(%d/%d)\"%(epoch+1, default_conf.epochs)+\"=\"*20)\n",
    "        for batch_i, (posX, negX) in enumerate(train_dataloader):\n",
    "            # Allocate tensor to devices\n",
    "               \n",
    "            posX = torch.LongTensor(posX).to(default_conf.device)\n",
    "            negX = torch.LongTensor(negX).to(default_conf.device)\n",
    "            # Normalize the embedding if neccessary\n",
    "            model.normalizeEmbedding()\n",
    "\n",
    "            # Calculate the loss from the model\n",
    "            loss = model(posX, negX)\n",
    "            lossVal = loss.cpu().item()\n",
    "\n",
    "            # Calculate the gradient and step down\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print infomation and add to summary\n",
    "            if minLoss > lossVal:\n",
    "                minLoss = lossVal\n",
    "            print(\"[TRAIN-EPOCH(%d/%d)-STEP(%d/%d)]Loss:%.4f, minLoss:%.4f\"%(epoch+1, default_conf.epochs, STEP,len(train_dataloader.dataset),lossVal, minLoss))\n",
    "            STEP += 1\n",
    "            GLOBALSTEP += 1\n",
    "            sumWriter.add_scalar('train/loss', lossVal, global_step=GLOBALSTEP)\n",
    "            \n",
    "            if batch_i == 5:\n",
    "                break\n",
    "\n",
    "        # if GLOBALEPOCH % default_conf.eval_epoch == 0:\n",
    "        #     MR = evaluation.MREvaluation(evalloader=valid_dataloader,\n",
    "        #                                     model=default_conf.model_name,\n",
    "        #                                     simMeasure=default_conf.sim_measure,\n",
    "        #                                     **model.retEvalWeights())\n",
    "        #     sumWriter.add_scalar('train/eval', MR, global_step=GLOBALEPOCH)\n",
    "        #     print(\"[EVALUATION-EPOCH(%d/%d)]Measure method %s, eval %.4f\"% \\\n",
    "        #             (epoch+1, default_conf.epochs, default_conf.evalmethod, MR))\n",
    "        #     # Save the model if new MR is better\n",
    "        #     if MR < bestMR:\n",
    "        #         bestMR = MR\n",
    "        #         model_path = os.path.join(default_conf.model_path, \"TransE_Model_MR_{}.pt\".format(round(bestMR,4)))\n",
    "        #         pkl_path = os.path.join(default_conf.embed_path, \"TransE_Embedding_MR_{}.pkl\".format(round(bestMR,4)))\n",
    "        #         torch.save(model.state_dict(), model_path)\n",
    "        #         with open(pkl_path, \"wb\") as fp:\n",
    "        #             pickle.dump({\"entlist\" : entity_rela_dict[\"Entity\"][\"stoi\"],\n",
    "        #                             \"rellist\" : entity_rela_dict[\"Rela\"][\"stoi\"],\n",
    "        #                             \"weights\" : model.retEvalWeights()}, fp)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
