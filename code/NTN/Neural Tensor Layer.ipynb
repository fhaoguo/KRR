{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cfa49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Loaded\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print ('Libraries Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731456e",
   "metadata": {},
   "source": [
    "![alt text](img1.png \"The Function modelled by the Neural Tensor Network\")\n",
    "\n",
    "This is the function modelled by the neural tensor layer where \n",
    "\n",
    "> * f is a standard nonlinearity applied element-wise, \n",
    "\n",
    "> * W<sup>[1:k]</sup><sub>R</sub>∈ R<sup>d×d×k</sup> is a tensor and the bilinear tensor product e<sup>T</sup><sub>1</sub>W<sup>[1:k]</sup><sub>R</sub>e2<sub>2</sub> results in a vector h ∈ R<sup>k</sup>, where each entry is computed by one slice i = 1, . . . , k of the tensor: hi = e<sup>T</sup><sub>1</sub>W<sup>[i]</sup><sub>R</sub>e2<sub>2</sub>. \n",
    "\n",
    "> * The other parameters for relation R are the standard form of a neural network: V<sub>R</sub>∈ R<sup>kx2d</sup>and U ∈ R<sup>k</sup>, b<sub>R</sub> ∈ R<sup>k</sup>.\n",
    "\n",
    "Four methods are required to be included in the class which will help to model the layer in keras :<br><br>\n",
    "* **__init()__** - This is used to initialise the layer.\n",
    "\n",
    "* **build(self, input_shape)** - Initialise the tensor variables and set the variables to be trained.\n",
    "\n",
    "* **call(self, x, mask=None)** - The forward pass operation is implemented here.\n",
    "\n",
    "* **get_output_shape_for(self, input_shape)** - Used to get the output shape before the network actually runs to help the building of the graph.\n",
    "\n",
    "![alt text](img2.png \"The Function modelled by the Neural Tensor Network\")\n",
    "\n",
    "**This is the block architecture of the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b0f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralTensorLayer(Layer):\n",
    "    def __init__(self, output_dim, input_dim, activation= None):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim #The k in the formula\n",
    "        self.input_dim = input_dim   #The d in the formula\n",
    "        self.activation = activation #The f function in the formula\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #The initialisation parameters\n",
    "        self.mean = 0.0 \n",
    "        self.stddev = 1.0\n",
    "        dtype = 'float32'\n",
    "        self.seed = 1\n",
    "        \n",
    "        #The output and the inut dimension\n",
    "        k = self.output_dim\n",
    "        d = self.input_dim\n",
    "        \n",
    "        #Initialise the variables to be trained. The variables are according to the\n",
    "        #function defined.\n",
    "        self.W = K.variable(K.random_normal((k,d,d), self.mean, self.stddev,\n",
    "                               dtype=dtype, seed=self.seed))\n",
    "        self.V = K.variable(K.random_normal((2*d,k), self.mean, self.stddev,\n",
    "                               dtype=dtype, seed=self.seed))\n",
    "        self.b = K.zeros((self.input_dim,))\n",
    "        \n",
    "        #Set the variables to be trained.\n",
    "        self._trainable_weights = [self.W, self.V, self.b]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        #Get Both the inputs\n",
    "        e1 = inputs[0]\n",
    "        e2 = inputs[1]\n",
    "        \n",
    "        #Get the batch size\n",
    "        batch_size = K.shape(e1)[0]\n",
    "        \n",
    "        #The output and the inut dimension\n",
    "        k = self.output_dim\n",
    "        d = self.input_dim\n",
    "\n",
    "        #The first term in the function which is the bilinear product is calculated here.\n",
    "        first_term_k = [K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1)]\n",
    "        for i in range(1, k):\n",
    "            temp = K.sum((e2 * K.dot(e1, self.W[i])) + self.b, axis=1)\n",
    "            first_term_k.append(temp)\n",
    "        first_term = K.reshape(K.concatenate(first_term_k, axis=0), (batch_size, k))\n",
    "\n",
    "        #The second term in the function is calculated here.\n",
    "        second_term = K.dot(K.concatenate([e1,e2]), self.V)\n",
    "        \n",
    "        #Sum of the two terms to get the final function\n",
    "        z =  first_term + second_term\n",
    "        \n",
    "        #The activation is selected here\n",
    "        if (self.activation == None):\n",
    "            return z\n",
    "        elif (self.activation == 'tanh'):\n",
    "            return K.tanh(z)\n",
    "        elif (self.activation == 'relu'):\n",
    "            return K.relu(z)\n",
    "        else :\n",
    "            print ('Activation not found')\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6cf2f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data:  (1000, 300) (1000, 300) (1000, 1)\n",
      "Shape of Validation Data (100, 300) (100, 300) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Dummy training data\n",
    "x_train1 = np.random.random((1000, 300))\n",
    "x_train2 = np.random.random((1000, 300))\n",
    "y_train = np.random.random((1000, 1))\n",
    "\n",
    "# Dummy validation data\n",
    "x_val1 = np.random.random((100, 300))\n",
    "x_val2 = np.random.random((100, 300))\n",
    "y_val = np.random.random((100, 1))\n",
    "\n",
    "print ('Shape of Training Data: ', x_train1.shape, x_train2.shape, y_train.shape)\n",
    "print ('Shape of Validation Data', x_val1.shape, x_val2.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b95efd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 20:34:17.871628: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " neural_tensor_layer (NeuralTen  (None, 32)          2899500     ['input_1[0][0]',                \n",
      " sorLayer)                                                        'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            33          ['neural_tensor_layer[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,899,533\n",
      "Trainable params: 2,899,533\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Here Define the model\n",
    "vector1 = Input(shape=(300,), dtype='float32')\n",
    "vector2 = Input(shape=(300,), dtype='float32')\n",
    "BilinearLayer = NeuralTensorLayer(output_dim=32, input_dim=300, \n",
    "                                  activation= 'relu')([vector1, vector2])\n",
    "\n",
    "#The g or the output of the modeled function.\n",
    "g = Dense(units = 1)(BilinearLayer)\n",
    "model = Model(inputs=[vector1, vector2], outputs=[g])\n",
    "\n",
    "#Compile the model\n",
    "adam = optimizers.Adam(lr = .001)\n",
    "model.compile( loss='mean_squared_error', optimizer=adam)\n",
    "#The summary of the model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf2a130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16/16 [==============================] - 5s 168ms/step - loss: 2423.8193 - val_loss: 266.8825\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 156.5361 - val_loss: 56.8615\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 20.8185 - val_loss: 9.0860\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 3.6241 - val_loss: 5.1959\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 4.1128 - val_loss: 4.0905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdafd02ab20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train1, x_train2], y_train,\n",
    "          batch_size=64, epochs=5,\n",
    "          validation_data=([x_val1, x_val2], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0aa9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
