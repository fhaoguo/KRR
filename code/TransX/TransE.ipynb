{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Generally, there are several steps as following:\n",
    "\n",
    "1. Transform raw data to standard format(CSV), some supported format lists as following(When dealing with the raw data, several exceptions should be considered):\n",
    "    1. There exists blank lines, inconsistent columns(!=3)\n",
    "    2. Head, relation, tail is nan or string that only contains space\n",
    "    3. Some string will be parsed as nan, exp.NAN, N/A, NA and so on\n",
    "2. Use data of standard format to generate entity and relation dict\n",
    "3. Split train, evaluation and test data if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# sys.path.append(\"../../\")\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "from config import Config\n",
    "\n",
    "from utils import utils\n",
    "from models import TransE, TransH, TransA, TransD, KG2E\n",
    "from utils import evaluation\n",
    "from dataloader.dataloader import tripleDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "default_conf = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_process(raw_path, save_path, names=None, header=None, sep=\"\\t\", encoding=\"utf-8\", compression=\"infer\"):\n",
    "    print(\"INFO : Loading data of type %s\" % os.path.splitext(raw_path)[-1])\n",
    "    raw_df = pd.read_csv(raw_path,\n",
    "                        sep=sep,\n",
    "                        encoding=encoding,\n",
    "                        names=names,\n",
    "                        header=header,\n",
    "                        keep_default_na=False,  # ==> Solve default nan\n",
    "                        compression=compression,# ==> Solve ZIP and TAR\n",
    "                        error_bad_lines=False,  # ==> Solve inconsistent lines\n",
    "                        warn_bad_lines=False,   # ==> Solve inconsistent lines\n",
    "                        skip_blank_lines=True)  # ==> Solve blank lines\n",
    "    print(\"INFO : Remove the space from the head and tail of entity.\")\n",
    "    raw_df = raw_df.applymap(lambda x: x.strip())  # Rid of the space in head and tail of entity\n",
    "    print(\"INFO : Drop line with nan value.\")    # Attention: \" \" should be removed.\n",
    "    raw_df.replace({'': np.nan}, inplace=True)\n",
    "    raw_df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    print(\"INFO : Save standard data to file path : %s\" % save_path)\n",
    "    raw_df.to_csv(save_path, sep=\"\\t\", header=None, index=None, encoding=\"utf-8\")\n",
    "    print(\"INFO : Successfully saving!\")\n",
    "    return raw_df\n",
    "\n",
    "\n",
    "def generate_dict(data_dfs, dict_save_dir):\n",
    "\n",
    "    raw_df = pd.concat(data_dfs, axis=0)\n",
    "    raw_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    head_counter = Counter(raw_df[\"head\"])\n",
    "    tail_counter = Counter(raw_df[\"tail\"])\n",
    "    rela_counter = Counter(raw_df[\"relation\"])\n",
    "\n",
    "    # Generate entity and relation list\n",
    "    entity_list = list((head_counter + tail_counter).keys())\n",
    "    rela_list = list(rela_counter.keys())\n",
    "\n",
    "    # Transform to index dict\n",
    "    print(\"INFO : Transform to index dict\")\n",
    "    entity_dict = dict([(word, ind) for ind, word in enumerate(entity_list)])\n",
    "    rela_dict = dict([(word, ind) for ind, word in enumerate(rela_list)])\n",
    "\n",
    "    # Save path\n",
    "    entity_dict_path = os.path.join(dict_save_dir, \"entity_dict.json\")\n",
    "    rela_dict_path = os.path.join(dict_save_dir, \"relation_dict.json\")\n",
    "\n",
    "    # Saving dicts\n",
    "    json.dump({\"stoi\": entity_dict, \"itos\": entity_list}, open(entity_dict_path, \"w\"))\n",
    "    json.dump({\"stoi\": rela_dict, 'itos': rela_list}, open(rela_dict_path, \"w\"))\n",
    "\n",
    "    return {\"Entity\": {\"stoi\": entity_dict, \"itos\": entity_list}, \n",
    "            \"Rela\": {\"stoi\": rela_dict, 'itos': rela_list} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Loading data of type .txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_5421/753348521.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  raw_df = pd.read_csv(raw_path,\n",
      "/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_5421/753348521.py:3: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  raw_df = pd.read_csv(raw_path,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Remove the space from the head and tail of entity.\n",
      "INFO : Drop line with nan value.\n",
      "INFO : Save standard data to file path : ../../data/TransX/train.txt\n",
      "INFO : Successfully saving!\n",
      "INFO : Loading data of type .txt\n",
      "INFO : Remove the space from the head and tail of entity.\n",
      "INFO : Drop line with nan value.\n",
      "INFO : Save standard data to file path : ../../data/TransX/valid.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_5421/753348521.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  raw_df = pd.read_csv(raw_path,\n",
      "/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_5421/753348521.py:3: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  raw_df = pd.read_csv(raw_path,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Successfully saving!\n",
      "INFO : Loading data of type .txt\n",
      "INFO : Remove the space from the head and tail of entity.\n",
      "INFO : Drop line with nan value.\n",
      "INFO : Save standard data to file path : ../../data/TransX/valid.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_5421/753348521.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  raw_df = pd.read_csv(raw_path,\n",
      "/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_5421/753348521.py:3: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  raw_df = pd.read_csv(raw_path,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Successfully saving!\n",
      "INFO : Transform to index dict\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dict_dir = \"../../data/TransX\"\n",
    "train_file = \"../../data/TransX/train.txt\"\n",
    "valid_file = \"../../data/TransX/valid.txt\"\n",
    "test_file = \"../../data/TransX/test.txt\"\n",
    "# Step1: Transform raw data to standard format\n",
    "train_df = csv_process(raw_path=train_file,\n",
    "                save_path=default_conf.pos_path,\n",
    "                names=[\"head\", \"relation\", \"tail\"],\n",
    "                header=None,\n",
    "                sep=\"\\t\",\n",
    "                encoding=\"utf-8\")\n",
    "valid_df = csv_process(raw_path=valid_file,\n",
    "                save_path=default_conf.valid_path,\n",
    "                names=[\"head\", \"relation\", \"tail\"],\n",
    "                header=None,\n",
    "                sep=\"\\t\",\n",
    "                encoding=\"utf-8\")\n",
    "test_df = csv_process(raw_path=test_file,\n",
    "                save_path=default_conf.test_path,\n",
    "                names=[\"head\", \"relation\", \"tail\"],\n",
    "                header=None,\n",
    "                sep=\"\\t\",\n",
    "                encoding=\"utf-8\")\n",
    "\n",
    "entity_rela_dict = generate_dict(data_dfs=[train_df, valid_df, test_df],\n",
    "                 dict_save_dir=dict_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Entity', 'Rela'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_rela_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1345, 14951)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_rela_dict[\"Rela\"][\"stoi\"]), len(entity_rela_dict[\"Entity\"][\"stoi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Load entity and relation dict.\n",
      "INFO : Loading positive triples and transform to index.\n",
      "INFO : Generate negtive samples from positive samples.\n",
      "INFO : Load entity and relation dict.\n",
      "INFO : Loading positive triples and transform to index.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize dataset and dataloader\n",
    "# If print(dataset[:]), you can get the result like:\n",
    "#   (np.array(N, 3, dtype=int64), np.array(N, 3, dtype=int64))\n",
    "# The first array represents the positive triples, while\n",
    "#   the second array represents the negtive ones.\n",
    "#   N is the size of all data.\n",
    "repSeed = 0\n",
    "exSeed = 0\n",
    "headSeed = 0\n",
    "tailSeed = 0\n",
    "\n",
    "train_dataset = tripleDataset(posDataPath=default_conf.pos_path,\n",
    "                    entityDictPath=default_conf.ent_path,\n",
    "                    relationDictPath=default_conf.rel_path)\n",
    "\n",
    "train_dataset.generateNegSamples(repProba=default_conf.rep_proba,\n",
    "                        exProba=default_conf.ex_proba,\n",
    "                        repSeed=repSeed,\n",
    "                        exSeed=exSeed,\n",
    "                        headSeed=headSeed,\n",
    "                        tailSeed=tailSeed)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                    batch_size=default_conf.batch_size,\n",
    "                    shuffle=default_conf.shuffle,\n",
    "                    num_workers=default_conf.num_workers,\n",
    "                    drop_last=default_conf.drop_last)\n",
    "\n",
    "\n",
    "valid_dataset = tripleDataset(posDataPath=default_conf.valid_path,\n",
    "                        entityDictPath=default_conf.ent_path,\n",
    "                        relationDictPath=default_conf.rel_path)\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                        batch_size=len(valid_dataset),\n",
    "                        shuffle=False,\n",
    "                        drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12986,   114, 12305],\n",
       "        [  863,   261,  3825],\n",
       "        [ 9493,    46,   427],\n",
       "        ...,\n",
       "        [ 4272,    73,   202],\n",
       "        [ 6031,    79,   496],\n",
       "        [  574,     8,   758]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  758,   114, 12305],\n",
       "        [  863,   261,  5635],\n",
       "        [ 9493,    46,  1693],\n",
       "        ...,\n",
       "        [ 4272,    73,  5533],\n",
       "        [ 6031,    79,   341],\n",
       "        [  574,     8,  4331]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_dataloader(seed):\n",
    "    epSeed = seed\n",
    "    exSeed = seed\n",
    "    headSeed = seed\n",
    "    tailSeed = seed\n",
    "\n",
    "    train_dataset = tripleDataset(posDataPath=default_conf.pos_path,\n",
    "                        entityDictPath=default_conf.ent_path,\n",
    "                        relationDictPath=default_conf.rel_path)\n",
    "\n",
    "    train_dataset.generateNegSamples(repProba=default_conf.rep_proba,\n",
    "                            exProba=default_conf.ex_proba,\n",
    "                            repSeed=repSeed,\n",
    "                            exSeed=exSeed,\n",
    "                            headSeed=headSeed,\n",
    "                            tailSeed=tailSeed)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                        batch_size=default_conf.batch_size,\n",
    "                        shuffle=default_conf.shuffle,\n",
    "                        num_workers=default_conf.num_workers,\n",
    "                        drop_last=default_conf.drop_last)\n",
    "    \n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def prepare_val_datset():\n",
    "    valid_dataset = tripleDataset(posDataPath=default_conf.valid_path,\n",
    "                        entityDictPath=default_conf.ent_path,\n",
    "                        relationDictPath=default_conf.rel_path)\n",
    "    valid_dataloader = DataLoader(valid_dataset,\n",
    "                            batch_size=len(valid_dataset),\n",
    "                            shuffle=False,\n",
    "                            drop_last=False)\n",
    "\n",
    "    return valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransE.TransE(entityNum=len(entity_rela_dict[\"Entity\"][\"stoi\"]),\n",
    "                    relationNum=len(entity_rela_dict[\"Rela\"][\"stoi\"]),\n",
    "                    embeddingDim=default_conf.TransE[\"EmbeddingDim\"],\n",
    "                    margin=default_conf.TransE[\"Margin\"],\n",
    "                    L=default_conf.TransE[\"L\"])\n",
    "model.to(default_conf.device)\n",
    "                    \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                weight_decay= default_conf.weight_decay,\n",
    "                                lr=default_conf.learning_rate)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=default_conf.lr_decay, patience=default_conf.lr_decay_epoch, threshold=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Using seed 0\n",
      "INFO : Load entity and relation dict.\n",
      "INFO : Loading positive triples and transform to index.\n",
      "INFO : Generate negtive samples from positive samples.\n",
      "====================EPOCHS(1/5)====================\n",
      "[TRAIN-EPOCH(1/5)-STEP(0/483142)]Loss:1.0075, minLoss:1.0075\n",
      "[TRAIN-EPOCH(1/5)-STEP(1/483142)]Loss:0.9963, minLoss:0.9963\n",
      "[TRAIN-EPOCH(1/5)-STEP(2/483142)]Loss:0.9981, minLoss:0.9963\n",
      "[TRAIN-EPOCH(1/5)-STEP(3/483142)]Loss:0.9885, minLoss:0.9885\n",
      "[TRAIN-EPOCH(1/5)-STEP(4/483142)]Loss:0.9849, minLoss:0.9849\n",
      "[TRAIN-EPOCH(1/5)-STEP(5/483142)]Loss:0.9815, minLoss:0.9815\n",
      "====================EPOCHS(2/5)====================\n",
      "[TRAIN-EPOCH(2/5)-STEP(0/483142)]Loss:0.9589, minLoss:0.9589\n",
      "[TRAIN-EPOCH(2/5)-STEP(1/483142)]Loss:0.9682, minLoss:0.9589\n",
      "[TRAIN-EPOCH(2/5)-STEP(2/483142)]Loss:0.9582, minLoss:0.9582\n",
      "[TRAIN-EPOCH(2/5)-STEP(3/483142)]Loss:0.9401, minLoss:0.9401\n",
      "[TRAIN-EPOCH(2/5)-STEP(4/483142)]Loss:0.9452, minLoss:0.9401\n",
      "[TRAIN-EPOCH(2/5)-STEP(5/483142)]Loss:0.9492, minLoss:0.9401\n",
      "====================EPOCHS(3/5)====================\n",
      "[TRAIN-EPOCH(3/5)-STEP(0/483142)]Loss:0.9315, minLoss:0.9315\n",
      "[TRAIN-EPOCH(3/5)-STEP(1/483142)]Loss:0.9333, minLoss:0.9315\n",
      "[TRAIN-EPOCH(3/5)-STEP(2/483142)]Loss:0.9309, minLoss:0.9309\n",
      "[TRAIN-EPOCH(3/5)-STEP(3/483142)]Loss:0.9182, minLoss:0.9182\n",
      "[TRAIN-EPOCH(3/5)-STEP(4/483142)]Loss:0.9279, minLoss:0.9182\n",
      "[TRAIN-EPOCH(3/5)-STEP(5/483142)]Loss:0.9216, minLoss:0.9182\n",
      "====================EPOCHS(4/5)====================\n",
      "[TRAIN-EPOCH(4/5)-STEP(0/483142)]Loss:0.9176, minLoss:0.9176\n",
      "[TRAIN-EPOCH(4/5)-STEP(1/483142)]Loss:0.9132, minLoss:0.9132\n",
      "[TRAIN-EPOCH(4/5)-STEP(2/483142)]Loss:0.8980, minLoss:0.8980\n",
      "[TRAIN-EPOCH(4/5)-STEP(3/483142)]Loss:0.8949, minLoss:0.8949\n",
      "[TRAIN-EPOCH(4/5)-STEP(4/483142)]Loss:0.8875, minLoss:0.8875\n",
      "[TRAIN-EPOCH(4/5)-STEP(5/483142)]Loss:0.8777, minLoss:0.8777\n",
      "====================EPOCHS(5/5)====================\n",
      "[TRAIN-EPOCH(5/5)-STEP(0/483142)]Loss:0.8953, minLoss:0.8777\n",
      "[TRAIN-EPOCH(5/5)-STEP(1/483142)]Loss:0.8796, minLoss:0.8777\n",
      "[TRAIN-EPOCH(5/5)-STEP(2/483142)]Loss:0.8855, minLoss:0.8777\n",
      "[TRAIN-EPOCH(5/5)-STEP(3/483142)]Loss:0.8708, minLoss:0.8708\n",
      "[TRAIN-EPOCH(5/5)-STEP(4/483142)]Loss:0.8711, minLoss:0.8708\n",
      "[TRAIN-EPOCH(5/5)-STEP(5/483142)]Loss:0.8724, minLoss:0.8708\n"
     ]
    }
   ],
   "source": [
    "sumWriter = SummaryWriter(log_dir=default_conf.summary_dir)\n",
    "\n",
    "# Training, GLOBALSTEP and GLOBALEPOCH are used for summary\n",
    "minLoss = float(\"inf\")\n",
    "bestMR = float(\"inf\")\n",
    "\n",
    "GLOBALSTEP = 0\n",
    "GLOBALEPOCH = 0\n",
    "\n",
    "for seed in range(1):\n",
    "    print(\"INFO : Using seed %d\" % seed)\n",
    "\n",
    "    train_dataloader = prepare_train_dataloader(seed)\n",
    "\n",
    "    for epoch in range(default_conf.epochs):\n",
    "        GLOBALEPOCH += 1\n",
    "        STEP = 0\n",
    "        print(\"=\"*20+\"EPOCHS(%d/%d)\"%(epoch+1, default_conf.epochs)+\"=\"*20)\n",
    "        for batch_i, (posX, negX) in enumerate(train_dataloader):\n",
    "            # Allocate tensor to devices\n",
    "               \n",
    "            posX = torch.LongTensor(posX).to(default_conf.device)\n",
    "            negX = torch.LongTensor(negX).to(default_conf.device)\n",
    "            # Normalize the embedding if neccessary\n",
    "            model.normalizeEmbedding()\n",
    "\n",
    "            # Calculate the loss from the model\n",
    "            loss = model(posX, negX)\n",
    "            lossVal = loss.cpu().item()\n",
    "\n",
    "            # Calculate the gradient and step down\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print infomation and add to summary\n",
    "            if minLoss > lossVal:\n",
    "                minLoss = lossVal\n",
    "            print(\"[TRAIN-EPOCH(%d/%d)-STEP(%d/%d)]Loss:%.4f, minLoss:%.4f\"%(epoch+1, default_conf.epochs, STEP,len(train_dataloader.dataset),lossVal, minLoss))\n",
    "            STEP += 1\n",
    "            GLOBALSTEP += 1\n",
    "            sumWriter.add_scalar('train/loss', lossVal, global_step=GLOBALSTEP)\n",
    "            \n",
    "            if batch_i == 5:\n",
    "                break\n",
    "\n",
    "        # if GLOBALEPOCH % default_conf.eval_epoch == 0:\n",
    "        #     MR = evaluation.MREvaluation(evalloader=valid_dataloader,\n",
    "        #                                     model=default_conf.model_name,\n",
    "        #                                     simMeasure=default_conf.sim_measure,\n",
    "        #                                     **model.retEvalWeights())\n",
    "        #     sumWriter.add_scalar('train/eval', MR, global_step=GLOBALEPOCH)\n",
    "        #     print(\"[EVALUATION-EPOCH(%d/%d)]Measure method %s, eval %.4f\"% \\\n",
    "        #             (epoch+1, default_conf.epochs, default_conf.evalmethod, MR))\n",
    "        #     # Save the model if new MR is better\n",
    "        #     if MR < bestMR:\n",
    "        #         bestMR = MR\n",
    "        #         model_path = os.path.join(default_conf.model_path, \"TransE_Model_MR_{}.pt\".format(round(bestMR,4)))\n",
    "        #         pkl_path = os.path.join(default_conf.embed_path, \"TransE_Embedding_MR_{}.pkl\".format(round(bestMR,4)))\n",
    "        #         torch.save(model.state_dict(), model_path)\n",
    "        #         with open(pkl_path, \"wb\") as fp:\n",
    "        #             pickle.dump({\"entlist\" : entity_rela_dict[\"Entity\"][\"stoi\"],\n",
    "        #                             \"rellist\" : entity_rela_dict[\"Rela\"][\"stoi\"],\n",
    "        #                             \"weights\" : model.retEvalWeights()}, fp)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}